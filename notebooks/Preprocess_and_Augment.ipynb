{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DeepNeuro](https://github.com/QTIM-Lab/DeepNeuro/raw/master/package_resources/logos/DeepNeuro_alt.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loading, Preprocessing, and Augmenting Data Using DeepNeuro\n",
    "\n",
    "In this notebook, you will learn how to load, preprocess, and augment data with DeepNeuro. DeepNeuro loads data from either structure folders, filepaths, or .csv files into DataCollection objects. DataCollection objects have pre-coded methods to perform data preprocessing, data augmentation, and patch extraction. They also can be automatically sampled for the purpose of training neural networks with DeepNeuro's DeepNeuroModel objects.\n",
    "\n",
    "You are going to get started creating a DataCollection object and applying some preprocessing steps. First, however, you need to get DeepNeuro installed, and import the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install deepneuro tensorflow-gpu keras\n",
    "\n",
    "import deepneuro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will work with two datasets from the TCGA-GBM dataset for this tutorial, available from The Cancer Imaging Archive (TCIA) and this link: [link](https://wiki.cancerimagingarchive.net/display/Public/TCGA-GBM).\n",
    "\n",
    "We will load this dataset using DeepNeuro's load module into a local folder in your CoLab directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/abeers/Github/DeepNeuro/deepneuro/load/Sample_Data/TCGA_GBM_NIFTI/TCGA_GBM_NIFTI.zip'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepneuro.load.load import load\n",
    "\n",
    "load('sample_gbm_nifti', output_datapath='./TCIA_GBM.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see two folders labeled 'TRAINING' and 'TESTING' contained within a folder titled 'TCGA_GBM_NIFTI'. Both of these folders contain individual patient folders, and each of these contain five NiFTi files each. Four of these files should code for different MR sequences of a patient with glioblastoma. The other file should be a segmentation file, containing voxelwise annotations for enhancing tumor, necrotic tissue, and edematous tissue in the brain.\n",
    "\n",
    "For most deep learning experiments, you should hopefully have more than three patients. For this tutorial, however, we will stick with three because it's faster to load data that way :). We will have two patients in the training set, and one patient in the test set.\n",
    "\n",
    "Let's start with our training data. Our first step will be to associate this two-patient dataset with a DataCollection, DeepNeuro's one-size-fits-all data processing object for data conversion, preprocessing, and augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gathering image data from...', {'TCGA_GBM_NIFTI/TRAINING': {'ground_truth': ['*GlistrBoost_ManuallyCorrected.nii.gz'], 'input_data': ['*_flair.nii.gz*', '*_t2.nii.gz*', '*_t1.nii.gz', '*_t1Gd.nii.gz']}}, '\\n')\n",
      "('WARNING: One of the data directories you have input,', 'TCGA_GBM_NIFTI/TRAINING', 'does not exist!')\n",
      "Found zero cases. Are you sure you have entered your data sources correctly?\n"
     ]
    }
   ],
   "source": [
    "from deepneuro.data.data_collection import DataCollection\n",
    "\n",
    "training_data_sources = {\n",
    "    'directories': {\n",
    "                'TCGA_GBM_NIFTI/TRAINING':\n",
    "                {'input_data': ['*_flair.nii.gz*', '*_t2.nii.gz*', '*_t1.nii.gz', '*_t1Gd.nii.gz'], \n",
    "                 'ground_truth': ['*GlistrBoost_ManuallyCorrected.nii.gz']}},\n",
    "}\n",
    "\n",
    "training_data = DataCollection(data_sources=training_data_sources, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it worked! But what does it mean to have your data in a DataCollection? We can start to get a sense using DeepNeuro's check_data function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-18fb31809a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepneuro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfigure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_collection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_mode_3d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2d_slice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m85\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/abeers/Github/DeepNeuro/deepneuro/utilities/visualize.py\u001b[0m in \u001b[0;36mcheck_data\u001b[0;34m(output_data, data_collection, batch_size, merge_batch, show_output, output_filepath, viz_rows, viz_mode_2d, viz_mode_3d, color_range, output_groups, combine_outputs, rgb_output, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperpetual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abeers/Github/DeepNeuro/deepneuro/data/data_collection.py\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(self, data_group_labels, perpetual, case_list, yield_data, verbose, batch_size, just_one_batch)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcase_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We start off with some code to making plotting in Juypter/Colab Notebooks work correctly.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "from deepneuro.utilities.visualize import check_data\n",
    "\n",
    "figure = check_data(data_collection=training_data, viz_rows=1, batch_size=2, viz_mode_3d='2d_slice', slice_index=85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the four sequences of our datasets labeled 0, 1, 2, and 3. We can also see our \"ground truth\", the multi-tissue tumor segmentation.\n",
    "\n",
    "So, we have our data available via a DataCollection. Can we start deep learning on it? No! Here are a few reasons:\n",
    "- As we can see from the intensity range bars in the figure above, MR patient data is scaled variably between 0 and 1000+. Because we don't know if they came from the same scanner or precisely the same sequence, we do not have assurances that these values will be on the same scale from patient to patient. Also, inputs to neural networks are conventional scaled to have a mean centered on zero and a standard deviation of 1, which our sequences certainly do not have.\n",
    "\n",
    "- Our ground truth is scaled from 0 to 4. Most deep learning cost functions for classification expect one-hot encoding. This means that each relevant classification is either 0 or 1, and that each class if separated into a different channel of the image. We will likely have training problems down the way if we do not adjust our data.\n",
    "\n",
    "- Can you train a machine learning algorithm on two pieces of data? Not likely! And even if you could, how are you going to fit a whole MRI scan into memory on anything but the most powerful GPUs? A common solution to both of these problems is to split your data into smaller patches, decreasing memory requirements and increasing effective dataset size. There are also other data augmentations that can be put into place to push the effective size of our data even higher.\n",
    "\n",
    "Luckily, DeepNeuro has utilities for performing each of these preprocessing steps and augmentations. We cover them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization -- MR Sequence Data\n",
    "\n",
    "MR images from different scanners and different sequences are not gauruanteed to have intensity values. To remedy this, we will perform a common preprocessing step in deep learning: zero-mean normalization. This preprocessing step simply ensures that all of our data will have a mean of 0, by subtracting each sequence's mean value from the total, and a variance of 1, by divided all intensities by resulting variance of the data.\n",
    "\n",
    "We accomplish this by creating a ZeroMeanNormalization object in DeepNeuro, and applying it to our data collection using the append_preprocessor command. We can check the data after we're done, and see if the normalization is reflected in the intensity bars on each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepneuro.preprocessing.signal import ZeroMeanNormalization\n",
    "\n",
    "normalization_preprocessor = ZeroMeanNormalization(data_groups=['input_data'], \n",
    "                                                   verbose=False, \n",
    "                                                   normalize_by_channel=True, \n",
    "                                                   mask_zeros=True)\n",
    "training_data.append_preprocessor(normalization_preprocessor)\n",
    "\n",
    "figure = check_data(data_collection=training_data, viz_rows=1, batch_size=2, viz_mode_3d='2d_slice', slice_index=85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformatting -- Voxelwise Segmentation Data\n",
    "\n",
    "When creating a neural network, one has a few options for cost functions. Some of these cost functions might require to reformat your ground-truth data, so that it can be properly calculated by your chosen deep learning package. One such example is the categorical cross-entropy loss, often used for multi-category (non-binary) classification problems. \n",
    "\n",
    "Categorical cross-entropy requires one-hot encoding for labels, which means label should a) occupy its own channel and b) be binary-valued (0/1). Unfortunately, our test data is all combined into one channel, and tissue values range from 0-4 (there is no '3' in this dataset, as this category was eliminated in this year's version of the dataset).\n",
    "\n",
    "To reformat our data, we will load the SplitData preprocessor object, and apply it to our training_data collection. We will make sure to specify that this only applies to our ground truth labels, and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepneuro.preprocessing.transform import SplitData\n",
    "\n",
    "one_hot_preprocessor = SplitData(data_groups=['ground_truth'], label_splits=[0, 1, 2, 4], verbose=False)\n",
    "training_data.append_preprocessor(one_hot_preprocessor)\n",
    "\n",
    "figure = check_data(data_collection=training_data, viz_rows=1, batch_size=2, viz_mode_3d='2d_slice', slice_index=85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation -- Patch Extraction\n",
    "\n",
    "Right now, we have datasets from two patients. Two patients does not a good training dataset make. Fortunately, we can augment our data using the ExtractPatches augmentation object in DeepNeuro.\n",
    "\n",
    "We are going to extract 200 patches from each of the patients in our training case, each 32x32x32 voxels, and train our model on those instead. We're also going to extract them in a specific way. If patches were extracted randomly from the entire space of the brain, there is a high liklihood that most of the patches would not overlap with cancerous tissue. For that reasons, we specify \"patch regions\". \n",
    "\n",
    "One region is the \"brain region\" which is everywhere inside the confines of the brain, but outside the our regions of interest. The other region is the \"roi region\", which is everywhere inside the tissues of interest. We can then control the ratio of these two types of patches we want to sample from. In this case, we will take 70% of our patches from the ROI region, and 30% of our patches from the brain region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepneuro.augmentation.subsample import ExtractPatches\n",
    "\n",
    "# Define patch sampling regions\n",
    "def brain_region(data):\n",
    "    return data['ground_truth'][..., 0] == 1\n",
    "\n",
    "def roi_region(data):\n",
    "    return data['ground_truth'][..., 1:] != 0\n",
    "\n",
    "# Add patch augmentation\n",
    "patch_augmentation = ExtractPatches(patch_shape=(32, 32, 32), \n",
    "                                    patch_region_conditions=[[brain_region, .3], [roi_region, .7]], \n",
    "                                    data_groups=['input_data', 'ground_truth'],\n",
    "                                    verbose=True)\n",
    "training_data.append_augmentation(patch_augmentation, multiplier=200)\n",
    "\n",
    "# We've dropped some of the parameters for check_data here. check_data's default behavior is to\n",
    "# grab the centermost axial slice in a given 3D volume. Earlier, that would result in an mostly\n",
    "# uninteresting slice in the cerrebellum, so we overrode that behavior with the slice_index and\n",
    "# viz_mode_3d parameters to get a more interesting slice to look at. For patches, we don't need\n",
    "# to do that.\n",
    "figure = check_data(data_collection=training_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation -- Rotations and Flips\n",
    "\n",
    "So 200x2 patients brings us up to 400 patches. But there are many other ways to augment data aside from simply divvying it up into patches. One simple way is to apply rotations and flips. Four 90 degree rotations with an optional flip effectively multiplies your data by 8 times, bringing up your total number of training patches to 3200.\n",
    "\n",
    "We apply a flip/rotation augmentation using DeepNeuro's Flip_Rotate_2D augmentation object below. We do our rotating and flipping in the axial plane, although other utilites exist for doing arbitrary rotations in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepneuro.augmentation.augment import Flip_Rotate_2D\n",
    "\n",
    "flip_augmentation = Flip_Rotate_2D(flip=True, \n",
    "                                   rotate=True, \n",
    "                                   data_groups=['input_data', 'ground_truth'], \n",
    "                                   flip_axis=2)\n",
    "training_data.append_augmentation(flip_augmentation, multiplier=8)\n",
    "\n",
    "figure = check_data(data_collection=training_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to HDF5\n",
    "\n",
    "So, to review, we now have 3200 pieces of data. DeepNeuro has the ability to do live augmentation during training, which means we do not actually have to create any HDF5s to train a neural network. However, for some steps in this pipeline -- particularly the patch extraction -- it will save time to pre-load patches, rather than always creating them on the fly. Furthermore, saving HDF5 data means that we can shuffle through the data efficiently when performing data augmentation, instead of having to load several different patients into memory to get a random sample.\n",
    "\n",
    "How do we save to HDF5 format? Pretty easily! DataCollections have a built-in function called write_data_to_file that dumps all available data, preprocessed and augmented, into a given HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_hdf5_file = 'training_brain_patches.hdf5'\n",
    "\n",
    "training_data.write_data_to_file(output_hdf5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from HDF5\n",
    "\n",
    "So say you want to write a new script in DeepNeuro, and load data from your old HDF5 file. Easy! Just create a new data collection, and set your data_sources parameter to your saved HDF5.\n",
    "\n",
    "We do so below, and use the check_data function to make sure everything looks alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_training_data = DataCollection(data_sources={'hdf5': 'training_brain_patches.hdf5'})\n",
    "\n",
    "figure = check_data(data_collection=loaded_training_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output is shuffled this time -- rotated and flipped patches are not found next to each other. The advantages of pre-loading data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And there you have it!\n",
    "\n",
    "You have preprocessed and augmented some MRI data, and are now ready to train a neural network on it. Find out how to do that on the other tutorial found at https://github.com/QTIM-Lab/DeepNeuro.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
